{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab528ae3-06a8-431d-ac22-c42372a429e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "In machine learning, polynomial functions and kernel functions are related through the use of polynomial kernels. A polynomial kernel is a type of kernel function that computes the dot product of the transformed feature vectors using a polynomial function. It allows linear algorithms to operate in a higher-dimensional space without explicitly calculating the transformed features.\n",
    "\n",
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "In this example, we use SVC from Scikit-learn with kernel='poly' to specify a polynomial kernel, and we set the degree of the polynomial using the degree parameter.\n",
    "\n",
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "In Support Vector Regression (SVR), epsilon (\n",
    "�\n",
    "ε) represents the margin of tolerance where no penalty is given to errors. Increasing the value of epsilon allows more data points to be within the margin of tolerance, resulting in a larger tube around the regression line.\n",
    "\n",
    "As epsilon increases, more data points are considered as support vectors, and the number of support vectors tends to increase. A larger epsilon allows for a broader margin and a more flexible regression model that accommodates more data points within the specified tolerance.\n",
    "\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "Kernel function: The choice of kernel function (linear, polynomial, radial basis function) determines the mapping of the input features into a higher-dimensional space. The appropriate kernel depends on the nature of the data. For example, a radial basis function (RBF) kernel is suitable for non-linear relationships.\n",
    "\n",
    "C parameter: C is the regularization parameter that controls the trade-off between achieving a low training error and a smooth decision boundary. A smaller C makes the decision boundary smoother (increased regularization), while a larger C allows the model to fit the training data more closely. Increase C when you want a more complex model that fits the training data closely.\n",
    "\n",
    "Epsilon parameter (for SVR): Epsilon (\n",
    "�\n",
    "ε) in SVR defines the margin of tolerance for errors. A larger epsilon allows more data points to be within the margin of tolerance. Increase epsilon when you want a broader margin and a more flexible regression model.\n",
    "\n",
    "Gamma parameter: Gamma (\n",
    "�\n",
    "γ) is a parameter for non-linear hyperplanes in the RBF kernel. A small gamma results in a large similarity radius, leading to a smoother decision boundary, while a large gamma makes the decision boundary more sensitive to variations in the training data. Increase gamma when you want a more complex and flexible model.\n",
    "\n",
    "Q5. Assignment:\n",
    "Below is a template to guide you through the implementation of an SVM classifier, hyperparameter tuning, and saving the trained classifier to a file.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'poly', 'rbf'], 'degree': [2, 3, 4], 'gamma': [0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print('Best Hyperparameters:', best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svm_classifier = SVC(**best_params)\n",
    "tuned_svm_classifier.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(tuned_svm_classifier, 'tuned_svm_classifier.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
